\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023
% ready for submission
\usepackage[preprint]{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}
\usepackage{commands}




\title{Expected Gradient Estimator With Many Environments}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Navid Ardeshir \\
  Department of Statistics\\
  Columbia University \\
  \texttt{navid.ardeshir@columbia.edu} \\
}


\begin{document}


\maketitle


\begin{abstract}
    In this paper we deal with the problem of finding an invariant subspace where the regression function only varies in that subspace across different environments.
    This problem can also be thought of as a representation learning with nonlinear link function.
    Inspired by ideas in the sufficient dimension reduction literature \citep{li2018sufficient} and recent developments in multi-task learning \citep{boursier2022trace} we propose a novel method which exploits data from all environments and provide numerical evidence for its tractability. 
\end{abstract}

\section{Introduction}
\input{intro.tex}

\section{Preliminaries}
\input{prelim.tex}

% \section{Related Works}
% \input{related.tex}

\input{analysis.tex}

\section{Conclusion}
In this paper we deal with the problem of reducing the dimenionality of the data and finding an invariant subspace among different environments.
This problem can be indeed challenging as the regression function for each environment is nonlinear and may differ arbitrarily. 
Our setting also have close ties to multi-task and meta learning and inspired by these literatures we propose a novel method for finding such subspaces. 
Finally we empirically validate our method and compare it with classical methods from sufficient dimension reduction literature.



\bibliographystyle{plainnat}
\bibliography{bibliography}

\section{Appendix}
\input{experiments.tex}




\end{document}